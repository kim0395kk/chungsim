# app.py â€” AI í–‰ì •ê´€ Pro (v6.0 / Agentic Dual-Model Router)
# Groq: qwen/qwen3-32b (FAST) + llama-3.3-70b-versatile (STRICT)
# LAWGO(DRF) + NAVER + Supabase
#
# í•µì‹¬ ê°œì„  (ì„±ëŠ¥/ì •í™•ë„ ê¸‰ìƒìŠ¹)
# âœ… (1) Extractor(ìŠ¬ë¡¯ì¶”ì¶œ) -> Candidate Law Search(í›„ë³´í’€) -> Law Selector(í›„ë³´ ì¤‘ ì„ íƒ) -> Verify(ì›ë¬¸í™•ë³´) -> Draft
# âœ… (2) ë²•ë ¹ íŠ ë°©ì§€: LLMì€ í›„ë³´ ëª©ë¡ì—ì„œë§Œ ì„ íƒ
# âœ… (3) ì¡°ë¬¸ ì›ë¬¸ í™•ë³´ ì‹¤íŒ¨ ì‹œ: "ë²•ë ¹ ë‹¨ì • ê¸ˆì§€" ëª¨ë“œë¡œ ê¸°ì•ˆ í”„ë¡¬í”„íŠ¸ ê°•ì œ
# âœ… (4) NAVERëŠ” ë³´ì¡° (On/Off + ê´€ë ¨ì„± í•„í„° + ì „ë¬¸ì„± í•„í„°)
# âœ… (5) ì¤‘ê°„ì— í•œì/ë¹„ì •ìƒ ë¬¸ì(U+EA01 ë“±) ì œê±°/ì •ë¦¬(ì…ë ¥/í‘œì‹œ ëª¨ë‘)
# âœ… (6) Metrics: ëª¨ë¸ë³„ í˜¸ì¶œ ìˆ˜ + total_tokens(ê°€ëŠ¥í•˜ë©´) + ë‹¨ê³„ë³„ ì¹´ìš´íŠ¸
# âœ… (7) Anti-crash: optional deps, timeouts, JSON retry/ìŠ¹ê¸‰, HTML sanitize, components.html ì•ˆì •í™”

import streamlit as st
import streamlit.components.v1 as components

import json
import re
import time
from datetime import datetime
from html import escape, unescape
from typing import Any

# =========================
# 0) Optional Imports (Safety)
# =========================
try:
    from groq import Groq
except Exception:
    Groq = None

try:
    import requests
except Exception:
    requests = None

try:
    import xmltodict
except Exception:
    xmltodict = None

try:
    from supabase import create_client
except Exception:
    create_client = None

# =========================
# 1) Page & Style
# =========================
st.set_page_config(
    layout="wide",
    page_title="AI í–‰ì •ê´€ Pro (Agentic v6.0)",
    page_icon="ğŸ›ï¸",
    initial_sidebar_state="collapsed",
)

st.markdown(
    """
<style>
.stApp { background-color: #f8f9fa; }

.paper-sheet{
  background:#fff; width:100%; max-width:210mm; min-height:297mm;
  padding:25mm; margin:auto; box-shadow:0 4px 15px rgba(0,0,0,.08);
  font-family:'Noto Serif KR','Nanum Myeongjo',serif; color:#111; line-height:1.6; position:relative;
}
.doc-header{ text-align:center; font-size:24pt; font-weight:900; margin-bottom:28px; letter-spacing:2px; }
.doc-info{
  display:flex; justify-content:space-between; gap:10px; flex-wrap:wrap;
  font-size:11pt; border-bottom:2px solid #333; padding-bottom:12px; margin-bottom:22px;
}
.doc-body{ font-size:12pt; text-align:justify; white-space:normal; }
.doc-footer{ text-align:center; font-size:22pt; font-weight:bold; margin-top:80px; letter-spacing:4px; }
.stamp{
  position:absolute; bottom:85px; right:80px; border:3px solid #d32f2f; color:#d32f2f;
  padding:6px 12px; font-size:14pt; font-weight:bold; transform:rotate(-15deg);
  opacity:.8; border-radius:4px; font-family:'Nanum Gothic', sans-serif;
}

.agent-log{
  font-family:'Pretendard',sans-serif; font-size:.9rem; padding:8px 12px;
  border-radius:8px; margin-bottom:6px; background:#fff; border:1px solid #e5e7eb;
}
.log-extract{ border-left:4px solid #0ea5e9; color:#0c4a6e; }
.log-law{ border-left:4px solid #3b82f6; color:#1e40af; }
.log-verify{ border-left:4px solid #22c55e; color:#166534; }
.log-search{ border-left:4px solid #f97316; color:#c2410c; }
.log-strat{ border-left:4px solid #8b5cf6; color:#6d28d9; }
.log-draft{ border-left:4px solid #ef4444; color:#991b1b; }
.log-sys{ border-left:4px solid #9ca3af; color:#374151; }

.small-muted{ color:#6b7280; font-size:12px; }
.badge{ display:inline-block; padding:3px 9px; border-radius:999px; font-size:12px; margin-right:6px; border:1px solid #e5e7eb; background:#fff; }
.badge-ok{ border-color:#bbf7d0; background:#f0fdf4; }
.badge-warn{ border-color:#fde68a; background:#fffbeb; }
.badge-bad{ border-color:#fecaca; background:#fef2f2; }

.item-card{ background:#fff; border:1px solid #e5e7eb; border-radius:12px; padding:12px 14px; margin-bottom:10px; }
.item-title{ font-weight:800; }
.item-meta{ color:#6b7280; font-size:12px; margin-top:4px; line-height:1.3; }
.item-desc{ margin-top:8px; white-space:pre-line; }
</style>
""",
    unsafe_allow_html=True,
)

_TAG_RE = re.compile(r"<[^>]+>")
# ì œì–´ë¬¸ì + Private Use Area(U+E000~U+F8FF) ì œê±°(ëŒ€í‘œì ìœ¼ë¡œ U+EA01 ê°™ì€ ê²ƒ)
_CTRL_RE = re.compile(r"[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]")
_PUA_RE = re.compile(r"[\uE000-\uF8FF]")

# =========================
# 2) Helpers (Sanitize / Parse)
# =========================
def clean_text(value: Any) -> str:
    """HTML íƒœê·¸/ì œì–´ë¬¸ì/PUA/ì´ìƒ ê³µë°± ì œê±°"""
    if value is None:
        return ""
    s = str(value)
    s = unescape(s)
    s = _TAG_RE.sub("", s)
    s = _CTRL_RE.sub("", s)
    s = _PUA_RE.sub("", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def safe_html(value: Any) -> str:
    return escape(clean_text(value), quote=False).replace("\n", "<br>")

def truncate_text(s: str, max_chars: int = 2500) -> str:
    s = s or ""
    return s if len(s) <= max_chars else s[:max_chars] + "\n...(ë‚´ìš© ì¶•ì†Œë¨)"

def safe_json_dump(obj: Any) -> str:
    try:
        return json.dumps(obj, ensure_ascii=False, default=str)
    except Exception:
        return "{}"

def ensure_doc_shape(doc: Any) -> dict:
    fallback = {
        "title": "ë¬¸ ì„œ (ìƒì„± ì‹¤íŒ¨)",
        "receiver": "ìˆ˜ì‹ ì ì°¸ì¡°",
        "body_paragraphs": ["ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•´ ë¬¸ì„œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."],
        "department_head": "í–‰ì •ê¸°ê´€ì¥",
    }
    if not isinstance(doc, dict):
        return fallback

    body = doc.get("body_paragraphs")
    if isinstance(body, str):
        body = [body]
    if not isinstance(body, list) or not body:
        body = fallback["body_paragraphs"]

    body_clean = [clean_text(x) for x in body if clean_text(x)]
    if not body_clean:
        body_clean = fallback["body_paragraphs"]

    return {
        "title": clean_text(doc.get("title") or fallback["title"]),
        "receiver": clean_text(doc.get("receiver") or fallback["receiver"]),
        "body_paragraphs": body_clean,
        "department_head": clean_text(doc.get("department_head") or fallback["department_head"]),
    }

def extract_keywords_kor(text: str, max_k: int = 8) -> list[str]:
    """LLM ì—†ì´ë„ í›„ë³´í’€ ë„“íˆëŠ” ì•ˆì „ë§"""
    if not text:
        return []
    t = re.sub(r"[^ê°€-í£A-Za-z0-9\s]", " ", text)
    words = re.findall(r"[ê°€-í£A-Za-z0-9]{2,14}", t)
    stop = set([
        "ê·¸ë¦¬ê³ ","ê´€ë ¨","ë¬¸ì˜","ì‚¬í•­","ëŒ€í•˜ì—¬","ëŒ€í•œ","ì²˜ë¦¬","ìš”ì²­","ì‘ì„±","ì•ˆë‚´","ê²€í† ","ë¶ˆí¸","ë¯¼ì›","ì‹ ì²­","ë°œê¸‰","ì œì¶œ",
        "ìœ„í•´","ëŒ€í•œ","ìŠµë‹ˆë‹¤","í•©ë‹ˆë‹¤","ì…ë‹ˆë‹¤","ê°€ëŠ¥","ì¡°ì¹˜","ëŒ€ìƒ","ê²½ìš°","í™•ì¸"
    ])
    out = []
    for w in words:
        if w in stop: 
            continue
        if w.isdigit():
            continue
        if w not in out:
            out.append(w)
        if len(out) >= max_k:
            break
    return out

def score_overlap(text: str, terms: list[str]) -> int:
    t = text or ""
    hit = 0
    for w in terms:
        if w and w in t:
            hit += 1
    return hit

# =========================
# 3) Metrics
# =========================
def metrics_init():
    if "metrics" not in st.session_state:
        st.session_state["metrics"] = {
            "calls": {},
            "tokens_total": 0,
            "steps": {"extract":0,"law_search":0,"law_select":0,"law_verify":0,"naver":0,"strategy":0,"draft":0}
        }

def metrics_add(model_name: str, tokens_total: int | None = None):
    metrics_init()
    m = st.session_state["metrics"]
    m["calls"][model_name] = m["calls"].get(model_name, 0) + 1
    if tokens_total is not None:
        try:
            m["tokens_total"] += int(tokens_total)
        except Exception:
            pass

def step_inc(step: str):
    metrics_init()
    st.session_state["metrics"]["steps"][step] = st.session_state["metrics"]["steps"].get(step, 0) + 1

metrics_init()

# =========================
# 4) LLM Service (Dual Router)
# =========================
class LLMService:
    """
    secrets.toml
    [general]
    GROQ_API_KEY = "..."
    GROQ_MODEL_FAST = "qwen/qwen3-32b"
    GROQ_MODEL_STRICT = "llama-3.3-70b-versatile"
    """
    def __init__(self):
        g = st.secrets.get("general", {})
        self.key = g.get("GROQ_API_KEY")
        self.model_fast = g.get("GROQ_MODEL_FAST", "qwen/qwen3-32b")
        self.model_strict = g.get("GROQ_MODEL_STRICT", "llama-3.3-70b-versatile")
        self.client = None
        self.last_model = "N/A"

        if Groq and self.key:
            try:
                self.client = Groq(api_key=self.key)
            except Exception:
                self.client = None

    def _chat(self, model: str, messages: list[dict], temp: float, json_mode: bool):
        if not self.client:
            raise RuntimeError("Groq client not ready")

        kwargs = {"model": model, "messages": messages, "temperature": temp}
        if json_mode:
            kwargs["response_format"] = {"type": "json_object"}

        resp = self.client.chat.completions.create(**kwargs)
        self.last_model = model

        tokens_total = None
        try:
            usage = getattr(resp, "usage", None)
            if usage:
                tokens_total = getattr(usage, "total_tokens", None)
        except Exception:
            tokens_total = None

        metrics_add(model, tokens_total=tokens_total)
        return resp.choices[0].message.content or ""

    def _parse_json(self, text: str) -> dict:
        try:
            return json.loads(text)
        except Exception:
            cleaned = re.sub(r"```json|```", "", text).strip()
            m = re.search(r"\{.*\}", cleaned, re.DOTALL)
            if m:
                try:
                    return json.loads(m.group(0))
                except Exception:
                    return {}
            return {}

    def generate_text(self, prompt: str, prefer: str = "fast", temp: float = 0.1) -> str:
        if not self.client:
            return "Groq API Keyê°€ ì—†ê±°ë‚˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜"

        model_first = self.model_fast if prefer == "fast" else self.model_strict
        messages = [
            {"role": "system", "content": "Korean public administration assistant. Be practical, concise, and correct."},
            {"role": "user", "content": prompt},
        ]
        # 1) first
        try:
            return self._chat(model_first, messages, temp, json_mode=False)
        except Exception:
            pass
        # 2) fallback strict
        try:
            return self._chat(self.model_strict, messages, temp, json_mode=False)
        except Exception as e:
            return f"LLM Error: {e}"

    def generate_json(self, prompt: str, prefer: str = "fast", temp: float = 0.1, max_retry: int = 2) -> dict:
        if not self.client:
            return {}

        sys_json = "Output JSON only. No markdown. No explanation. Follow the schema exactly."
        messages = [
            {"role": "system", "content": sys_json},
            {"role": "user", "content": prompt},
        ]
        model_first = self.model_fast if prefer == "fast" else self.model_strict

        # 1) same model retry
        for _ in range(max_retry):
            try:
                txt = self._chat(model_first, messages, temp, json_mode=True)
                js = self._parse_json(txt)
                if js:
                    return js
            except Exception:
                pass

        # 2) upgrade to strict
        try:
            txt = self._chat(self.model_strict, messages, temp, json_mode=True)
            js = self._parse_json(txt)
            return js if js else {}
        except Exception:
            return {}

llm = LLMService()

# =========================
# 5) LAW API (DRF)
# =========================
class LawAPIService:
    """
    secrets.toml
    [law]
    LAW_API_ID = "OCê°’"
    """
    def __init__(self):
        self.oc = st.secrets.get("law", {}).get("LAW_API_ID")
        self.search_url = "https://www.law.go.kr/DRF/lawSearch.do"
        self.service_url = "https://www.law.go.kr/DRF/lawService.do"
        self.enabled = bool(requests and xmltodict and self.oc)

    def search_law(self, query: str, display: int = 10) -> list[dict]:
        if not self.enabled or not query:
            return []
        try:
            params = {"OC": self.oc, "target": "law", "type": "XML", "query": query, "display": display, "page": 1}
            r = requests.get(self.search_url, params=params, timeout=7)
            r.raise_for_status()
            data = xmltodict.parse(r.text)

            laws = data.get("LawSearch", {}).get("law", [])
            if isinstance(laws, dict):
                laws = [laws]

            out = []
            for it in laws:
                if not isinstance(it, dict):
                    continue
                nm = it.get("ë²•ë ¹ëª…í•œê¸€") or it.get("lawNm") or it.get("ë²•ë ¹ëª…") or ""
                mst = it.get("ë²•ë ¹ì¼ë ¨ë²ˆí˜¸") or it.get("MST") or it.get("mst") or ""
                link = it.get("ë²•ë ¹ìƒì„¸ë§í¬") or it.get("link") or ""
                out.append({
                    "law_name": clean_text(nm),
                    "mst": clean_text(mst),
                    "link": clean_text(link),
                })
            out = [x for x in out if x["law_name"]]
            return out
        except Exception:
            return []

    def get_article_text_by_mst(self, mst: str, article_no: str | None = None) -> str:
        if not self.enabled or not mst:
            return ""
        try:
            params = {"OC": self.oc, "target": "law", "type": "XML", "MST": mst}
            r = requests.get(self.service_url, params=params, timeout=9)
            r.raise_for_status()
            data = xmltodict.parse(r.text)

            law = data.get("Law") or data.get("law") or {}
            articles = law.get("Article", []) or []
            if isinstance(articles, dict):
                articles = [articles]

            # ì¡°ë¬¸ë²ˆí˜¸ ì—†ìœ¼ë©´ ì¼ë¶€ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜(í‘œì‹œ/LLMì°¸ì¡°ìš©)
            if not article_no:
                raw = clean_text(r.text)
                return raw[:4000]

            tgt = re.sub(r"[^0-9]", "", str(article_no))
            if not tgt:
                return ""

            for art in articles:
                if not isinstance(art, dict):
                    continue
                an = clean_text(art.get("@ì¡°ë¬¸ë²ˆí˜¸") or "")
                at = clean_text(art.get("ArticleTitle") or "")
                an_num = re.sub(r"[^0-9]", "", an)

                if tgt == an_num or (tgt and f"ì œ{tgt}ì¡°" in at):
                    content = clean_text(art.get("ArticleContent") or "")
                    paras = art.get("Paragraph", [])
                    if isinstance(paras, dict):
                        paras = [paras]
                    p_text = "\n".join([clean_text(p.get("ParagraphContent")) for p in paras if isinstance(p, dict)])
                    return "\n".join([x for x in [at, content, p_text] if x]).strip()

            return ""
        except Exception:
            return ""

law_api = LawAPIService()

# =========================
# 6) NAVER Search (Optional)
# =========================
class NaverSearchService:
    """
    secrets.toml
    [naver]
    CLIENT_ID="..."
    CLIENT_SECRET="..."
    """
    BASE = "https://openapi.naver.com/v1/search"
    _PRO = [
        "ë²•ë ¹","ì‹œí–‰ë ¹","ì‹œí–‰ê·œì¹™","ì¡°ë¬¸","íŒë¡€","í–‰ì •ì‹¬íŒ","í–‰ì •ì†Œì†¡","ê³¼íƒœë£Œ","ì²˜ë¶„","ì‚¬ì „í†µì§€",
        "ì˜ê²¬ì œì¶œ","ì´ì˜ì‹ ì²­","ë¶ˆë³µ","ìœ ê¶Œí•´ì„","ì§ˆì˜íšŒì‹ ","ê³ ì‹œ","í›ˆë ¹","ì˜ˆê·œ","ì§€ì¹¨","ë§¤ë‰´ì–¼","ê°€ì´ë“œ",
        "ë²•ì œì²˜","êµ­ê°€ë²•ë ¹ì •ë³´","í–‰ì •ì ˆì°¨ë²•","ê°œì¸ì •ë³´","ë³´í˜¸ë²•","ìš”ê±´","ê¸°ì¤€"
    ]
    _NONPRO = ["í›„ê¸°","ë§›ì§‘","ì¼ìƒ","ì—¬í–‰","ë‹¤ì´ì–´íŠ¸","ë¸Œì´ë¡œê·¸","ë‚´ëˆë‚´ì‚°","ê°ì„±","ì—°ì• ","ìœ¡ì•„","ë¦¬ë·°"]

    def __init__(self):
        n = st.secrets.get("naver", {})
        self.cid = n.get("CLIENT_ID")
        self.csec = n.get("CLIENT_SECRET")
        self.enabled = bool(requests and self.cid and self.csec)

    def _call(self, endpoint: str, query: str, display: int = 8, sort: str = "sim"):
        if not self.enabled or not query:
            return None
        try:
            url = f"{self.BASE}/{endpoint}.json"
            headers = {"X-Naver-Client-Id": self.cid, "X-Naver-Client-Secret": self.csec}
            params = {"query": query, "display": display, "start": 1, "sort": sort}
            r = requests.get(url, headers=headers, params=params, timeout=7)
            r.raise_for_status()
            return r.json()
        except Exception:
            return None

    @classmethod
    def professional_score(cls, title: str, desc: str, link: str) -> int:
        t = (title or "") + " " + (desc or "")
        score = 0
        for k in cls._PRO:
            if k in t:
                score += 2
        if re.search(r"ì œ?\s*\d+\s*ì¡°", t):
            score += 4
        if len(desc or "") >= 80:
            score += 1
        for k in cls._NONPRO:
            if k in t:
                score -= 4
        if re.search(r"[ğŸ˜‚ğŸ¤£ğŸ˜ğŸ˜…]|ã…‹ã…‹|ã…ã…|ã… ã… ", t):
            score -= 2
        if any(dom in (link or "") for dom in ["law.go.kr", "go.kr", "ac.kr", "korea.kr"]):
            score += 3
        return score

    def parse_items(self, data: dict, source: str) -> list[dict]:
        out = []
        if not data:
            return out
        for it in (data.get("items") or [])[:15]:
            title = clean_text(it.get("title", "")) or "(ì œëª© ì—†ìŒ)"
            desc = clean_text(it.get("description", "")) or clean_text(it.get("snippet", ""))
            link = clean_text(it.get("link", ""))
            out.append({"source": source, "title": title, "desc": truncate_text(desc, 320), "link": link})
        # dedup
        uniq, seen = [], set()
        for x in out:
            key = x["link"] or (x["source"] + "|" + x["title"])
            if key in seen:
                continue
            seen.add(key)
            uniq.append(x)
        return uniq

    def search_bundle(self, terms: list[str], primary_law: str, display_news=8, display_web=8, display_blog=12, display_cafe=12) -> list[dict]:
        if not self.enabled:
            return []
        step_inc("naver")

        terms = [t for t in terms if t]
        terms_q = " ".join(terms[:6]) if terms else ""
        if not terms_q:
            return []

        q_news = f"{terms_q} í–‰ì •ì²˜ë¶„ ì‚¬ë¡€"
        q_web  = f"{terms_q} ë²•ë ¹ í•´ì„¤"
        q_blog = f"{terms_q} ì‹¤ë¬´ í•´ì„¤"
        q_cafe = f"{terms_q} ì§ˆì˜íšŒì‹ "

        news = self._call("news", q_news, display=display_news)
        webkr = self._call("webkr", q_web, display=display_web)
        blog = self._call("blog", q_blog, display=display_blog)
        cafe = self._call("cafearticle", q_cafe, display=display_cafe)

        items = []
        items += self.parse_items(news, "news")
        items += self.parse_items(webkr, "webkr")
        items += self.parse_items(blog, "blog")
        items += self.parse_items(cafe, "cafe")

        # relevance: ìµœì†Œ 2ê°œ í‚¤ì›Œë“œ íˆíŠ¸
        scored = []
        for x in items:
            t = (x["title"] or "") + " " + (x["desc"] or "")
            rel_hit = score_overlap(t, terms)
            rel_score = rel_hit * 3 - (6 if rel_hit < 2 else 0)

            pro_score = 0
            if x["source"] in ("blog","cafe"):
                pro_score = self.professional_score(x["title"], x["desc"], x["link"])

            x2 = dict(x)
            x2["rel_score"] = rel_score
            x2["pro_score"] = pro_score
            scored.append(x2)

        filtered = []
        for x in scored:
            if x["source"] in ("news","webkr"):
                if x["rel_score"] >= 6:
                    filtered.append(x)
            else:
                if x["rel_score"] >= 6 and x["pro_score"] >= 8:
                    filtered.append(x)

        filtered.sort(key=lambda z: (z.get("rel_score",0) + z.get("pro_score",0)*0.35), reverse=True)

        # cap per source
        caps = {"news":5,"webkr":5,"blog":3,"cafe":3}
        cnt = {k:0 for k in caps}
        out = []
        for x in filtered:
            s = x["source"]
            if s in caps and cnt[s] >= caps[s]:
                continue
            cnt[s] += 1
            out.append(x)
        return out

naver = NaverSearchService()

# =========================
# 7) Supabase
# =========================
class DatabaseService:
    """
    secrets.toml
    [supabase]
    SUPABASE_URL="..."
    SUPABASE_KEY="..."
    """
    def __init__(self):
        self.client = None
        s = st.secrets.get("supabase", {})
        url = s.get("SUPABASE_URL")
        key = s.get("SUPABASE_KEY")
        if create_client and url and key:
            try:
                self.client = create_client(url, key)
            except Exception:
                self.client = None

    def save_log(self, table: str, payload: dict) -> str:
        if not self.client:
            return "DB ë¯¸ì—°ê²°"
        try:
            safe_payload = json.loads(safe_json_dump(payload))
            self.client.table(table).insert(safe_payload).execute()
            return "ì €ì¥ ì„±ê³µ"
        except Exception as e:
            return f"ì €ì¥ ì‹¤íŒ¨: {e}"

db = DatabaseService()

# =========================
# 8) Agentic Pipeline (Extractor -> Candidates -> Selector -> Verify -> Strategy -> Draft)
# =========================
def extractor_slots(user_input: str) -> dict:
    """
    FAST: ë¯¼ì› ì„œì‚¬ -> ìŠ¬ë¡¯ êµ¬ì¡°í™”
    """
    step_inc("extract")
    user_input = clean_text(user_input)
    kw = extract_keywords_kor(user_input, max_k=10)

    prompt = f"""
ë„ˆëŠ” 'ë¯¼ì›/ì—…ë¬´ ìƒí™©'ì„ ë²•ë¥  ê²€í† ì— ìœ ë¦¬í•œ êµ¬ì¡°ë¡œ ë¶„í•´í•œë‹¤.
ì•„ë˜ ìŠ¤í‚¤ë§ˆë¥¼ ì •í™•íˆ ì§€ì¼œ JSONë§Œ ì¶œë ¥:

{{
  "object": "ëŒ€ìƒ(ì˜ˆ: ê±´ì„¤ê¸°ê³„/ìë™ì°¨/ë„ë¡œ/ì£¼ì°¨/ì˜ì—…/ë³µì§€ì¡°ì‚¬ ë“±)",
  "act": "í•µì‹¬ í–‰ìœ„(ì˜ˆ: ë°©ì¹˜/ë¶ˆë²•ì£¼ì°¨/ì°¨ê³ ì§€ ì™¸ ì£¼ì°¨/ë¯¸ì´í–‰/ë¶ˆë²•ì˜ì—…/ê³¼íƒœë£Œ ì´ì˜ ë“±)",
  "place": "ì¥ì†Œ(ëª¨ë¥´ë©´ ë¹ˆë¬¸ìì—´)",
  "time": "ì‹œê°„/ê¸°ê°„/ë°˜ë³µ(ëª¨ë¥´ë©´ ë¹ˆë¬¸ìì—´)",
  "request": "ìš”ì²­/ëª©í‘œ(ì˜ˆ: ì‹œì •ìš”ì²­/ë‹¨ì†ìš”ì²­/ì²˜ë¶„ì·¨ì†Œ/ì•ˆë‚´ë¬¸ ì‘ì„± ë“±)",
  "agency_scope": "ë‹´ë‹¹ì(ì§€ìì²´)ê°€ í•  ìˆ˜ ìˆëŠ” ë²”ìœ„/ì²˜ë¦¬ìœ í˜•(ëª¨ë¥´ë©´ ë¹ˆë¬¸ìì—´)",
  "keywords": ["í‚¤ì›Œë“œ1","í‚¤ì›Œë“œ2","í‚¤ì›Œë“œ3","í‚¤ì›Œë“œ4","í‚¤ì›Œë“œ5"]
}}

ê·œì¹™:
- ë‚´ìš© ëª¨ë¥´ë©´ ë¹ˆë¬¸ìì—´.
- keywordsëŠ” 'ëª…ì‚¬ ìœ„ì£¼', 5ê°œ ì´í•˜.
- ë²•ë ¹ëª…/ì¡°ë¬¸ë²ˆí˜¸ëŠ” ì—¬ê¸°ì„œ ì“°ì§€ ë§ˆ.

[ë¯¼ì›]
{user_input}

[í‚¤ì›Œë“œ íŒíŠ¸(ë£° ê¸°ë°˜)]
{kw[:10]}
"""
    js = llm.generate_json(prompt, prefer="fast", max_retry=2)
    if not js:
        return {
            "object": "",
            "act": "",
            "place": "",
            "time": "",
            "request": "",
            "agency_scope": "",
            "keywords": kw[:5],
        }

    # sanitize fields
    out = {}
    out["object"] = clean_text(js.get("object",""))
    out["act"] = clean_text(js.get("act",""))
    out["place"] = clean_text(js.get("place",""))
    out["time"] = clean_text(js.get("time",""))
    out["request"] = clean_text(js.get("request",""))
    out["agency_scope"] = clean_text(js.get("agency_scope",""))
    kws = js.get("keywords", [])
    if not isinstance(kws, list):
        kws = []
    kws = [clean_text(x) for x in kws if clean_text(x)]
    if not kws:
        kws = kw[:5]
    out["keywords"] = kws[:5]
    return out

def build_law_queries(slots: dict, user_input: str) -> list[str]:
    """
    ë£° ê¸°ë°˜ìœ¼ë¡œ ë²•ë ¹ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ë…¸ì´ì¦ˆ ì¤„ì´ê¸°)
    """
    base = []
    for k in (slots.get("keywords") or []):
        if k and k not in base:
            base.append(k)
    obj_ = slots.get("object","")
    act_ = slots.get("act","")
    req_ = slots.get("request","")

    combos = []
    if obj_ and act_:
        combos.append(f"{obj_} {act_}")
    if act_:
        combos.append(act_)
    if obj_:
        combos.append(obj_)
    if obj_ and req_:
        combos.append(f"{obj_} {req_}")
    if act_ and req_:
        combos.append(f"{act_} {req_}")

    # í‚¤ì›Œë“œ 1~2ê°œ ì¡°í•©
    if len(base) >= 2:
        combos.append(f"{base[0]} {base[1]}")
    if base:
        combos.append(base[0])

    # ë§ˆì§€ë§‰ ì•ˆì „ë§: ì…ë ¥ì—ì„œ ì¶”ì¶œ
    fallback = extract_keywords_kor(user_input, max_k=6)
    for f in fallback:
        combos.append(f)

    # dedup + trim
    out, seen = [], set()
    for q in combos:
        q = clean_text(q)
        if not q:
            continue
        if q in seen:
            continue
        seen.add(q)
        out.append(q)
        if len(out) >= 10:
            break
    return out

def law_candidate_pool(user_input: str, slots: dict, pool_size: int = 25) -> list[dict]:
    """
    DRFë¡œ í›„ë³´í’€ í™•ì¥ (LLM ì—†ì´ë„ ì‘ë™)
    """
    step_inc("law_search")
    queries = build_law_queries(slots, user_input)
    pool = []
    for q in queries[:8]:
        pool += law_api.search_law(q, display=10)

    # dedup (mst+law_name)
    uniq, seen = [], set()
    for x in pool:
        key = (x.get("mst","") + "|" + x.get("law_name","")).strip()
        if not key or key in seen:
            continue
        seen.add(key)
        uniq.append(x)
        if len(uniq) >= pool_size:
            break
    return uniq

def law_selector_from_pool(user_input: str, slots: dict, pool: list[dict]) -> dict:
    """
    STRICT: í›„ë³´ ëª©ë¡ì—ì„œë§Œ ëŒ€í‘œë²•ë ¹/ì—°ê´€ë²•ë ¹ ì„ íƒ + ì¡°ë¬¸ 'ìˆ«ìë§Œ'(ê°€ëŠ¥í•˜ë©´)
    """
    step_inc("law_select")
    if not pool:
        return {
            "primary": None,
            "related": [],
            "article_no": "",
            "status": "FAIL",
            "reason": "í›„ë³´í’€ ì—†ìŒ",
        }

    # í›„ë³´ í…ìŠ¤íŠ¸ (ì§§ê²Œ)
    lines = []
    for i, it in enumerate(pool[:25], 1):
        lines.append(f"{i}. {it.get('law_name')} (MST={it.get('mst')})")
    pool_text = "\n".join(lines)

    prompt = f"""
ë„ˆëŠ” ëŒ€í•œë¯¼êµ­ í–‰ì • ì‹¤ë¬´ 'ë²•ë ¹ ì„ íƒê¸°'ë‹¤.
ì•„ë˜ [í›„ë³´ ëª©ë¡] ì¤‘ì—ì„œë§Œ ì„ íƒí•œë‹¤. ì ˆëŒ€ ëª©ë¡ ë°– ë²•ë ¹ì„ ë§Œë“¤ì§€ ë§ˆ.

ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥:
{{
  "pick": {{
    "primary_idx": 1,
    "related_idx": [2,3],
    "article_no": "ì¡°ë²ˆí˜¸(ìˆ«ìë§Œ, ëª¨ë¥´ë©´ ë¹ˆë¬¸ìì—´)"
  }},
  "reason": "í•œ ë¬¸ì¥"
}}

[ë¯¼ì› ì›ë¬¸]
{clean_text(user_input)}

[ìŠ¬ë¡¯ ìš”ì•½]
- ëŒ€ìƒ(object): {slots.get("object","")}
- í–‰ìœ„(act): {slots.get("act","")}
- ìš”ì²­(request): {slots.get("request","")}
- ì²˜ë¦¬ë²”ìœ„(scope): {slots.get("agency_scope","")}
- í‚¤ì›Œë“œ: {slots.get("keywords",[])}

[í›„ë³´ ëª©ë¡]
{pool_text}

ê·œì¹™:
- primary_idxëŠ” 1~{min(len(pool),25)} ì¤‘ 1ê°œ
- related_idxëŠ” 0~2ê°œ(ì—†ìœ¼ë©´ [])
- article_noëŠ” ìˆ«ìë§Œ. í™•ì‹  ì—†ìœ¼ë©´ ë¹ˆë¬¸ìì—´.
"""
    js = llm.generate_json(prompt, prefer="strict", max_retry=2) or {}
    pick = js.get("pick", {})
    if not isinstance(pick, dict):
        pick = {}

    pidx = pick.get("primary_idx")
    rids = pick.get("related_idx", [])
    art = clean_text(pick.get("article_no",""))

    # normalize indices
    def valid_idx(x):
        return isinstance(x, int) and 1 <= x <= len(pool)

    primary = pool[pidx-1] if valid_idx(pidx) else pool[0]
    related = []

    if isinstance(rids, list):
        for rid in rids:
            if valid_idx(rid):
                cand = pool[rid-1]
                if cand not in related and cand != primary:
                    related.append(cand)
            if len(related) >= 2:
                break

    # ensure at least 3 total when possible
    all_list = [primary] + related
    for it in pool:
        if it not in all_list:
            all_list.append(it)
        if len(all_list) >= 3:
            break

    # article number (digits only)
    art_num = re.sub(r"[^0-9]", "", art)

    return {
        "primary": all_list[0] if all_list else None,
        "related": all_list[:3],
        "article_no": art_num,
        "status": "OK" if all_list else "FAIL",
        "reason": clean_text(js.get("reason","")),
    }

def law_verify_fetch(primary: dict | None, article_no: str) -> dict:
    """
    ì›ë¬¸ í™•ë³´ ê¸°ì¤€ìœ¼ë¡œ CONFIRMED/WEAK ê²°ì •
    """
    step_inc("law_verify")
    if not primary:
        return {"status": "FAIL", "legal_basis": "ê´€ë ¨ ë²•ë ¹ ê²€ìƒ‰ ì‹¤íŒ¨(í›„ë³´ ì—†ìŒ).", "article_text": "", "link": ""}

    nm = clean_text(primary.get("law_name",""))
    mst = clean_text(primary.get("mst",""))
    link = clean_text(primary.get("link",""))

    # ì¡°ë¬¸ ì›ë¬¸ ì‹œë„
    article_text = ""
    if mst:
        article_text = law_api.get_article_text_by_mst(mst, article_no if article_no else None)

    article_text = clean_text(article_text)

    if article_no and article_text and len(article_text) >= 40:
        legal_basis = f"{nm} ì œ{article_no}ì¡°\n{truncate_text(article_text, 2600)}"
        return {"status": "CONFIRMED", "legal_basis": legal_basis, "article_text": article_text, "link": link, "mst": mst, "law_name": nm}
    if (not article_no) and article_text and len(article_text) >= 60:
        legal_basis = f"{nm}\n{truncate_text(article_text, 2600)}"
        return {"status": "WEAK", "legal_basis": legal_basis, "article_text": article_text, "link": link, "mst": mst, "law_name": nm}

    # ì›ë¬¸ í™•ë³´ ì‹¤íŒ¨
    return {"status": "FAIL", "legal_basis": f"ë²•ë ¹({nm})ì€ ì°¾ì•˜ìœ¼ë‚˜ ì¡°ë¬¸ ì›ë¬¸ í™•ë³´ ì‹¤íŒ¨.", "article_text": "", "link": link, "mst": mst, "law_name": nm}

def strategy_agent(user_input: str, slots: dict, law_pack: dict, naver_items: list[dict]) -> str:
    """
    FAST ê¸°ë³¸. ë²•ë ¹ì´ FAILì´ë©´ STRICTë¡œ ìŠ¹ê¸‰.
    """
    step_inc("strategy")
    prefer = "fast" if law_pack.get("status") == "CONFIRMED" else "strict"

    brief = []
    for it in (naver_items or [])[:8]:
        brief.append(f"- [{it.get('source')}] {it.get('title')}: {it.get('desc')}")
    brief_block = "\n".join(brief) if brief else "(ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)"

    prompt = f"""
[ì¶œë ¥ ì œì•½]
- ì¸ì‚¿ë§/ìê¸°ì†Œê°œ ê¸ˆì§€. ë°”ë¡œ ë³¸ë¬¸.
- ê³¼ë„í•œ ì¼ë°˜ë¡  ê¸ˆì§€. ë³¸ ë¯¼ì›ê³¼ ë²•ë ¹/ì ˆì°¨ì— ì§ì ‘ ì—°ê²°ëœ ë¬¸ì¥ë§Œ.
- ì•„ë˜ 3ê°œ í•­ëª©ë§Œ, ë§ˆí¬ë‹¤ìš´.

[ë¯¼ì›]
{clean_text(user_input)}

[ìŠ¬ë¡¯]
- ëŒ€ìƒ: {slots.get("object","")}
- í–‰ìœ„: {slots.get("act","")}
- ìš”ì²­: {slots.get("request","")}
- ì²˜ë¦¬ë²”ìœ„: {slots.get("agency_scope","")}
- í‚¤ì›Œë“œ: {slots.get("keywords",[])}

[ë²•ì  ê·¼ê±° ìƒíƒœ]
{law_pack.get("status")}

[ë²•ì  ê·¼ê±°(í™•ë³´ ë²”ìœ„)]
{law_pack.get("legal_basis")}

[ë„¤ì´ë²„(ë³´ì¡°)]
{truncate_text(brief_block, 1100)}

1. **ì²˜ë¦¬ ë°©í–¥**
2. **í•µì‹¬ ì²´í¬ë¦¬ìŠ¤íŠ¸**
3. **ì˜ˆìƒ ë¯¼ì›/ë°˜ë°œ ë° ëŒ€ì‘**
"""
    return llm.generate_text(prompt, prefer=prefer, temp=0.1)

def draft_agent(dept: str, officer: str, user_input: str, slots: dict, law_pack: dict, strategy: str) -> dict:
    """
    STRICT: ê³µë¬¸ JSON ìƒì„±
    ë²•ë ¹ FAIL/WEAKì´ë©´ 'ì¶”ê°€ í™•ì¸ í•„ìš”' ë¬¸êµ¬ ê°•ì œ
    """
    step_inc("draft")
    today_str = datetime.now().strftime("%Y. %m. %d.")
    doc_num = f"í–‰ì •-{datetime.now().strftime('%Y')}-{int(time.time()) % 10000:04d}í˜¸"

    law_status = law_pack.get("status","FAIL")
    caution = ""
    if law_status != "CONFIRMED":
        caution = "â€» ë³¸ ë¬¸ì„œì˜ ë²•ì  ê·¼ê±°ëŠ” ì „ì‚°ì¡°íšŒ/ì›ë¬¸í™•ë³´ í•œê³„ë¡œ 'ì¶”ê°€ í™•ì¸ í•„ìš”'ê°€ í¬í•¨ë˜ì–´ì•¼ í•œë‹¤."

    prompt = f"""
ì•„ë˜ ìŠ¤í‚¤ë§ˆë¡œë§Œ JSON ì¶œë ¥(í‚¤ ì¶”ê°€ ê¸ˆì§€):
{{
  "title": "ë¬¸ì„œ ì œëª©",
  "receiver": "ìˆ˜ì‹ ",
  "body_paragraphs": ["ë¬¸ë‹¨1","ë¬¸ë‹¨2","ë¬¸ë‹¨3","ë¬¸ë‹¨4"],
  "department_head": "ë°œì‹  ëª…ì˜"
}}

ì‘ì„± ì •ë³´:
- ë¶€ì„œ: {clean_text(dept)}
- ë‹´ë‹¹ì: {clean_text(officer)}
- ì‹œí–‰ì¼: {today_str}
- ë¬¸ì„œë²ˆí˜¸: {doc_num}

ë¯¼ì›/ì—…ë¬´ ìƒí™©:
{clean_text(user_input)}

ìŠ¬ë¡¯ ìš”ì•½:
- ëŒ€ìƒ: {slots.get("object","")}
- í–‰ìœ„: {slots.get("act","")}
- ìš”ì²­: {slots.get("request","")}
- ì²˜ë¦¬ë²”ìœ„: {slots.get("agency_scope","")}
- í‚¤ì›Œë“œ: {slots.get("keywords",[])}

ë²•ì  ê·¼ê±° ìƒíƒœ: {law_status}
ë²•ì  ê·¼ê±°(í™•ë³´ ë²”ìœ„):
{law_pack.get("legal_basis")}

ì²˜ë¦¬ ì „ëµ(ìš”ì•½):
{truncate_text(clean_text(strategy), 1000)}

í•„ìˆ˜ ì›ì¹™:
- ë¬¸ì„œ í†¤: ê±´ì¡°/ì •ì¤‘, ë¶ˆí•„ìš”í•œ ìˆ˜ì‚¬ ê¸ˆì§€
- ë³¸ë¬¸ êµ¬ì¡°: [ê²½ìœ„] -> [ê·¼ê±°] -> [ì¡°ì¹˜/ì•ˆë‚´] -> [ê¶Œë¦¬êµ¬ì œ/ë¬¸ì˜]
- ê°œì¸ì •ë³´ëŠ” OOOë¡œ ë§ˆìŠ¤í‚¹(ìˆìœ¼ë©´)
- ë²•ë ¹ ì›ë¬¸ì´ ë¶ˆí™•ì‹¤í•˜ë©´ ë°˜ë“œì‹œ "ì¶”ê°€ í™•ì¸ í•„ìš”" ë˜ëŠ” "ì „ì‚° í™•ì¸ ê²°ê³¼" í‘œí˜„ì„ í¬í•¨
{caution}
"""
    js = llm.generate_json(prompt, prefer="strict", max_retry=2)
    doc = ensure_doc_shape(js)
    # ë©”íƒ€ëŠ” ë³„ë„ë¡œ ë°˜í™˜
    return {"doc": doc, "meta": {"doc_num": doc_num, "today": today_str}}

# =========================
# 9) Rendering
# =========================
def badge(text: str, kind: str = "ok") -> str:
    cls = "badge badge-ok" if kind == "ok" else ("badge badge-warn" if kind == "warn" else "badge badge-bad")
    return f"<span class='{cls}'>{escape(text)}</span>"

def render_precedents(items: list[dict]):
    if not items:
        st.info("ê´€ë ¨ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    def src_label(src: str) -> str:
        return {"news":"ë‰´ìŠ¤","webkr":"ì›¹ë¬¸ì„œ","blog":"ë¸”ë¡œê·¸(í•„í„°)","cafe":"ì¹´í˜(í•„í„°)"}.get(src, src or "ê²€ìƒ‰")

    for it in items[:16]:
        src = clean_text(it.get("source",""))
        title = clean_text(it.get("title",""))
        desc = clean_text(it.get("desc",""))
        link = clean_text(it.get("link",""))
        rel = it.get("rel_score")
        pro = it.get("pro_score")

        st.markdown("<div class='item-card'>", unsafe_allow_html=True)
        st.markdown(f"<div class='item-title'>[{escape(src_label(src))}] {escape(title)}</div>", unsafe_allow_html=True)

        meta = []
        if isinstance(rel, int):
            meta.append(f"rel={rel}")
        if isinstance(pro, int) and src in ("blog","cafe"):
            meta.append(f"pro={pro}")
        if meta:
            st.markdown(f"<div class='item-meta'>{escape(' | '.join(meta))}</div>", unsafe_allow_html=True)

        st.markdown(f"<div class='item-desc'>{escape(desc)}</div>", unsafe_allow_html=True)
        if link.startswith("http"):
            st.link_button("ì—´ê¸°", link, use_container_width=True)
        st.markdown("</div>", unsafe_allow_html=True)

def render_metrics():
    m = st.session_state.get("metrics", {})
    calls = m.get("calls", {})
    steps = m.get("steps", {})
    tokens_total = m.get("tokens_total", 0)

    st.subheader("ğŸ“Š ì‚¬ìš©ëŸ‰(ì„¸ì…˜ ê¸°ì¤€)")
    if calls:
        for k, v in sorted(calls.items(), key=lambda x: (-x[1], x[0])):
            st.write(f"- **{k}**: {v}íšŒ")
        st.caption(f"ì´ í† í°(ê°€ëŠ¥í•œ ê²½ìš°): {tokens_total}")
    else:
        st.info("í˜¸ì¶œ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("#### ğŸ§© ë‹¨ê³„ë³„ ì‹¤í–‰ íšŸìˆ˜")
    st.json(steps)

# =========================
# 10) Workflow Orchestrator
# =========================
def run_workflow(user_input: str, dept: str, officer: str, use_naver: bool):
    log_area = st.empty()
    logs = []

    def add_log(msg: str, style: str = "sys"):
        logs.append(f"<div class='agent-log log-{style}'>{safe_html(msg)}</div>")
        log_area.markdown("".join(logs), unsafe_allow_html=True)
        time.sleep(0.06)

    user_input = clean_text(user_input)

    # A) Extractor (FAST)
    add_log("ğŸ§  [Extractor] ë¯¼ì› ì„œì‚¬ë¥¼ ìŠ¬ë¡¯ìœ¼ë¡œ ë¶„í•´(FAST: qwen/qwen3-32b)...", "extract")
    slots = extractor_slots(user_input)

    # B) Candidate Law Search (DRF)
    add_log("ğŸ“š [LawSearch] DRFë¡œ í›„ë³´ ë²•ë ¹ í’€ ìƒì„±(ë£° ê¸°ë°˜ ì¿¼ë¦¬ + ë‹¤ì¤‘ ê²€ìƒ‰)...", "law")
    pool = law_candidate_pool(user_input, slots, pool_size=30)

    # C) Law Selector (STRICT, í›„ë³´ ì¤‘ ì„ íƒ)
    add_log("ğŸ¯ [LawSelect] í›„ë³´ ëª©ë¡ì—ì„œë§Œ ëŒ€í‘œ/ì—°ê´€ ë²•ë ¹ ì„ íƒ(STRICT: llama-3.3-70b)...", "law")
    selection = law_selector_from_pool(user_input, slots, pool)

    primary = selection.get("primary")
    related = selection.get("related", []) or []
    article_no = selection.get("article_no","")

    # D) Verify / Fetch article text
    add_log("âœ… [Verify] ëŒ€í‘œ ë²•ë ¹ ì›ë¬¸(ì¡°ë¬¸) í™•ë³´ë¡œ ì‹ ë¢°ë„ í™•ì •...", "verify")
    law_ver = law_verify_fetch(primary, article_no)

    # ìµœì¢… ë²•ë ¹ íŒ©
    law_pack = {
        "status": law_ver.get("status","FAIL"),
        "primary": primary,
        "related": related[:3],
        "article_no": article_no,
        "legal_basis": law_ver.get("legal_basis",""),
        "article_text": law_ver.get("article_text",""),
        "reason": selection.get("reason",""),
    }

    # E) Naver (optional)
    naver_items = []
    if use_naver and naver.enabled:
        add_log("ğŸ” [Naver] ìœ ì‚¬ ì‚¬ë¡€/í•´ì„¤ ê²€ìƒ‰(ë³´ì¡°, í•„í„° ì ìš©)...", "search")
        terms = []
        terms += [slots.get("object",""), slots.get("act",""), slots.get("request","")]
        terms += (slots.get("keywords") or [])
        terms = [t for t in [clean_text(x) for x in terms] if t]
        # dedup
        uniq = []
        seen = set()
        for t in terms:
            if t in seen:
                continue
            seen.add(t)
            uniq.append(t)
        naver_items = naver.search_bundle(uniq[:12], clean_text(primary.get("law_name","") if primary else ""))
    else:
        add_log("ğŸ” [Naver] OFF (ë¹„í™œì„±/í‚¤ ì—†ìŒ/ì‚¬ìš©ì ì˜µì…˜)", "search")

    # F) Strategy
    add_log("ğŸ§­ [Strategy] ì²˜ë¦¬ ë°©í–¥/ì²´í¬ë¦¬ìŠ¤íŠ¸/ëŒ€ì‘ ìˆ˜ë¦½...", "strat")
    strategy = strategy_agent(user_input, slots, law_pack, naver_items)

    # G) Draft (STRICT)
    add_log("âœï¸ [Draft] ê³µë¬¸ì„œ JSON ìƒì„±(STRICT: llama-3.3-70b)...", "draft")
    drafted = draft_agent(dept, officer, user_input, slots, law_pack, strategy)
    doc = drafted["doc"]
    meta = drafted["meta"]

    # H) Save
    add_log("ğŸ’¾ [Save] Supabase ì €ì¥...", "sys")
    payload = {
        "created_at": datetime.now().isoformat(),
        "dept": clean_text(dept),
        "officer": clean_text(officer),
        "input": user_input,
        "slots": safe_json_dump(slots),
        "law_status": law_pack["status"],
        "law_primary": safe_json_dump(primary or {}),
        "law_related": safe_json_dump(related[:3]),
        "law_article_no": article_no,
        "legal_basis": law_pack["legal_basis"],
        "strategy": strategy,
        "naver_items": safe_json_dump(naver_items),
        "final_doc": safe_json_dump(doc),
        "model_last": llm.last_model,
        "metrics": safe_json_dump(st.session_state.get("metrics", {})),
    }
    db_msg = db.save_log("law_logs", payload)  # í…Œì´ë¸”ëª…: law_logs (ì›í•˜ëŠ” ê±¸ë¡œ ë°”ê¾¸ë©´ ë¨)
    add_log(f"âœ… ì™„ë£Œ ({db_msg})", "sys")

    time.sleep(0.35)
    log_area.empty()

    return {
        "slots": slots,
        "pool_count": len(pool),
        "law_pack": law_pack,
        "naver_items": naver_items,
        "strategy": strategy,
        "doc": doc,
        "meta": meta,
        "db_msg": db_msg,
    }

# =========================
# 11) UI
# =========================
def main():
    st.session_state.setdefault("dept", "OOì‹œì²­ OOê³¼")
    st.session_state.setdefault("officer", "ê¹€ì£¼ë¬´ê´€")
    st.session_state.setdefault("use_naver", True)

    col_l, col_r = st.columns([1, 1.2], gap="large")

    with col_l:
        st.title("ğŸ›ï¸ AI í–‰ì •ê´€ Pro")
        st.caption("Agentic v6.0 â€” Extractor â†’ Candidate Pool â†’ Selector(í›„ë³´ì¤‘ì„ íƒ) â†’ Verify(ì›ë¬¸í™•ë³´) â†’ Draft")
        st.markdown("---")

        with st.expander("ğŸ“ ì‚¬ìš©ì ì •ë³´ / ì˜µì…˜", expanded=False):
            st.text_input("ë¶€ì„œëª…", key="dept")
            st.text_input("ë‹´ë‹¹ì", key="officer")
            st.checkbox("ë„¤ì´ë²„ ê²€ìƒ‰ ì‚¬ìš©(ë³´ì¡°)", key="use_naver")

        user_input = st.text_area(
            "ì—…ë¬´ ì§€ì‹œ ì‚¬í•­(ë¯¼ì› ìƒí™©)",
            height=220,
            placeholder="ì˜ˆ: ë¬´ë‹¨ë°©ì¹˜ì°¨ëŸ‰ ê°•ì œì²˜ë¦¬ ì ˆì°¨ ì•ˆë‚´ ê³µë¬¸ ì‘ì„±\nì˜ˆ: ê±´ì„¤ê¸°ê³„ ì°¨ê³ ì§€ ì™¸ ì£¼ì°¨(ì£¼ê¸°ìœ„ë°˜) ë¯¼ì› ë‹µë³€ë¬¸ ì‘ì„±",
        )

        if st.button("ğŸš€ ì‹¤í–‰", type="primary", use_container_width=True):
            if not clean_text(user_input):
                st.warning("ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”.")
            else:
                with st.spinner("ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘..."):
                    try:
                        res = run_workflow(
                            user_input=user_input,
                            dept=st.session_state["dept"],
                            officer=st.session_state["officer"],
                            use_naver=st.session_state["use_naver"],
                        )
                        st.session_state["result"] = res
                    except Exception as e:
                        st.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")

        st.markdown("---")
        render_metrics()

        st.markdown(
            "<div class='small-muted'>"
            "TIP: ì„±ëŠ¥ì´ íŠ€ë©´(ë²•ë ¹ ì—‰ëš±) â†’ í›„ë³´í’€/ì„ íƒ/ì›ë¬¸í™•ë³´ 3ë‹¨ê³„ê°€ ë°©ì–´í•©ë‹ˆë‹¤. "
            "ì¡°ë¬¸ ì›ë¬¸ í™•ë³´ ì‹¤íŒ¨ ì‹œ ê¸°ì•ˆì— 'ì¶”ê°€ í™•ì¸ í•„ìš”'ê°€ ìë™ í¬í•¨ë©ë‹ˆë‹¤."
            "</div>",
            unsafe_allow_html=True,
        )

    with col_r:
        res = st.session_state.get("result")

        if not res:
            st.markdown(
                """
<div style='text-align: center; padding: 120px 20px; color: #aaa; border: 2px dashed #ddd; border-radius: 12px; background:#fff;'>
  <h3>ğŸ“„ Document Preview</h3>
  <p>ì™¼ìª½ì—ì„œ ë¯¼ì› ìƒí™©ì„ ì…ë ¥í•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”.<br>ë²•ë ¹ í›„ë³´í’€/ê²€ì¦ í›„ ê³µë¬¸ì´ ìƒì„±ë©ë‹ˆë‹¤.</p>
</div>
""",
                unsafe_allow_html=True,
            )
        else:
            doc = res["doc"]
            meta = res["meta"]
            law_pack = res["law_pack"]

            tab1, tab2 = st.tabs(["ğŸ“„ ê³µë¬¸ì„œ", "ğŸ” ê·¼ê±°/ë¶„ì„"])

            with tab1:
                body_html = "".join([f"<p style='margin:0 0 14px 0;'>{safe_html(p)}</p>" for p in doc["body_paragraphs"]])
                html = f"""
<div class="paper-sheet">
  <div class="stamp">ì§ì¸ìƒëµ</div>
  <div class="doc-header">{safe_html(doc['title'])}</div>
  <div class="doc-info">
    <span>ë¬¸ì„œë²ˆí˜¸: {safe_html(meta['doc_num'])}</span>
    <span>ì‹œí–‰ì¼ì: {safe_html(meta['today'])}</span>
    <span>ìˆ˜ì‹ : {safe_html(doc['receiver'])}</span>
  </div>
  <div class="doc-body">{body_html}</div>
  <div class="doc-footer">{safe_html(doc['department_head'])}</div>
</div>
"""
                components.html(html, height=880, scrolling=True)

            with tab2:
                # ìƒíƒœ ë±ƒì§€
                st.markdown(
                    badge(f"DB: {clean_text(res.get('db_msg',''))}", "ok" if "ì„±ê³µ" in (res.get("db_msg","")) else "warn")
                    + badge(f"ë²•ë ¹ìƒíƒœ: {clean_text(law_pack.get('status'))}", "ok" if law_pack.get("status")=="CONFIRMED" else ("warn" if law_pack.get("status")=="WEAK" else "bad"))
                    + badge(f"í›„ë³´í’€: {res.get('pool_count',0)}ê±´", "ok"),
                    unsafe_allow_html=True,
                )

                st.markdown("### ğŸ§© ìŠ¬ë¡¯(Extractor ê²°ê³¼)")
                st.json(res.get("slots", {}))

                st.markdown("### ğŸ“œ ë²•ì  ê·¼ê±°(í™•ë³´ ë²”ìœ„)")
                st.info(law_pack.get("legal_basis",""))

                st.markdown("### ğŸ§­ ì²˜ë¦¬ ì „ëµ")
                st.markdown(res.get("strategy",""))

                if st.session_state.get("use_naver"):
                    st.markdown("### ğŸ” ë„¤ì´ë²„(ë³´ì¡°) â€” ê´€ë ¨ì„±/ì „ë¬¸ì„± í•„í„°")
                    render_precedents(res.get("naver_items", []))
                else:
                    st.caption("ë„¤ì´ë²„ ê²€ìƒ‰ OFF")

                with st.expander("ğŸ› ï¸ ë””ë²„ê·¸(ë²•ë ¹ ì„ íƒ/ì›ë¬¸í™•ë³´)", expanded=False):
                    dbg = {
                        "status": law_pack.get("status"),
                        "article_no": law_pack.get("article_no"),
                        "primary": law_pack.get("primary"),
                        "related": law_pack.get("related"),
                        "selector_reason": law_pack.get("reason"),
                    }
                    st.code(safe_json_dump(dbg), language="json")


if __name__ == "__main__":
    main()
